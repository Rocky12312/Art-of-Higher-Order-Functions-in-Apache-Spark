{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically what we will be doing is that we will be slicing and dicing the nested JSON stucture(used by the NoSQL databases) using spark SQL functions as there exists ways to do that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The higher order functions are primarily suited for manipulating the arrays in spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now creating a JSON schema with attributes and values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is the way of defining the schema in pyspark(First entry(name),second entry(datatype),third entry(nullable or not))\n",
    "\"\"\"schema = StructType([StructField('Name', StringType(), True),\n",
    "                     StructField('DateTime', TimestampType(), True)\n",
    "                     StructField('Age', IntegerType(), True)])\"\"\"\n",
    "\n",
    "#This the basic JSON schema(actually the json documents appear in this format)\n",
    "\"\"\"{\n",
    "  \"$schema\": \"http://json-schema.org/draft/2019-09/schema\",\n",
    "  \"title\": \"Product\",\n",
    "  \"type\": \"object\",\n",
    "  \"required\": [\"id\", \"name\", \"price\"],\n",
    "  \"properties\": {\n",
    "    \"id\": {\n",
    "      \"type\": \"number\",\n",
    "      \"description\": \"Product identifier\"\n",
    "    },\n",
    "    \"name\": {\n",
    "      \"type\": \"string\",\n",
    "      \"description\": \"Name of the product\"\n",
    "    },\n",
    "    \"price\": {\n",
    "      \"type\": \"number\",\n",
    "      \"minimum\": 0\n",
    "    },\n",
    "    \"tags\": {\n",
    "      \"type\": \"array\",\n",
    "      \"items\": {\n",
    "        \"type\": \"string\"\n",
    "      }\n",
    "    },\n",
    "    \"stock\": {\n",
    "      \"type\": \"object\",\n",
    "      \"properties\": {\n",
    "        \"warehouse\": {\n",
    "          \"type\": \"number\"\n",
    "        },\n",
    "        \"retail\": {\n",
    "          \"type\": \"number\"\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\"\"\"\n",
    "#Dealing with JSON data format\n",
    "#Creating the JSON schema\n",
    "schema = StructType().add(\"dc_id\", StringType()).add(\"source\", MapType(StringType(), StructType()\n",
    "                        .add(\"description\", StringType())\n",
    "                        .add(\"ip\", StringType())\n",
    "                        .add(\"id\", IntegerType())\n",
    "                        .add(\"temp\", ArrayType(IntegerType()))\n",
    "                        .add(\"c02_level\", ArrayType(IntegerType()))\n",
    "                        .add(\"geo\", StructType()\n",
    "                             .add(\"lat\", DoubleType())\n",
    "                             .add(\"long\", DoubleType()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting the JSON string th the dataframe(second citizer of spark)@also parallelizing\n",
    "def JSON_To_DataFrame(json, schema=None):\n",
    "    reader = spark.read\n",
    "    if schema:\n",
    "        reader.schema(schema)\n",
    "    return reader.json(sc.parallelize([json]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[dc_id: string, source: map<string,struct<description:string,ip:string,id:int,temp:array<int>,c02_level:array<int>,geo:struct<lat:double,long:double>>>]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Taking a JSON document and converting it to dataframe using the above function\n",
    "#Giving input json document and the schema schema structure we created\n",
    "#we can give the format of json as we want(according to our necessity)\n",
    "Data_Frame = JSON_To_DataFrame(\"\"\"{\n",
    "\n",
    "    \"dc_id\": \"dc-101\",\n",
    "    \"source\": {\n",
    "        \"sensor-igauge\": {\n",
    "        \"id\": 10,\n",
    "        \"ip\": \"68.28.91.22\",\n",
    "        \"description\": \"Sensor attached to the container ceilings\",\n",
    "        \"temp\":[35,35,35,36,35,35,32,35,30,35,32,35],\n",
    "        \"c02_level\": [1475,1476,1473],\n",
    "        \"geo\": {\"lat\":38.00, \"long\":97.00}                        \n",
    "      },\n",
    "      \"sensor-ipad\": {\n",
    "        \"id\": 13,\n",
    "        \"ip\": \"67.185.72.1\",\n",
    "        \"description\": \"Sensor ipad attached to carbon cylinders\",\n",
    "        \"temp\": [45,45,45,46,45,45,42,35,40,45,42,45],\n",
    "        \"c02_level\": [1370,1371,1372],\n",
    "        \"geo\": {\"lat\":47.41, \"long\":-122.00}\n",
    "      },\n",
    "      \"sensor-inest\": {\n",
    "        \"id\": 8,\n",
    "        \"ip\": \"208.109.163.218\",\n",
    "        \"description\": \"Sensor attached to the factory ceilings\",\n",
    "        \"temp\": [40,40,40,40,40,43,42,40,40,45,42,45],\n",
    "        \"c02_level\": [1346,1345, 1343],\n",
    "        \"geo\": {\"lat\":33.61, \"long\":-111.89}\n",
    "      },\n",
    "      \"sensor-istick\": {\n",
    "        \"id\": 5,\n",
    "        \"ip\": \"204.116.105.67\",\n",
    "        \"description\": \"Sensor embedded in exhaust pipes in the ceilings\",\n",
    "        \"temp\":[30,30,30,30,40,43,42,40,40,35,42,35],\n",
    "        \"c02_level\": [1574,1570, 1576],\n",
    "        \"geo\": {\"lat\":35.93, \"long\":-85.46}\n",
    "      }\n",
    "    }\n",
    "  }\"\"\",schema)\n",
    "#displaying the dataframe data format\n",
    "display(Data_Frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "| dc_id|              source|\n",
      "+------+--------------------+\n",
      "|dc-101|[sensor-igauge ->...|\n",
      "+------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Data_Frame.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dc_id: string (nullable = true)\n",
      " |-- source: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: struct (valueContainsNull = true)\n",
      " |    |    |-- description: string (nullable = true)\n",
      " |    |    |-- ip: string (nullable = true)\n",
      " |    |    |-- id: integer (nullable = true)\n",
      " |    |    |-- temp: array (nullable = true)\n",
      " |    |    |    |-- element: integer (containsNull = true)\n",
      " |    |    |-- c02_level: array (nullable = true)\n",
      " |    |    |    |-- element: integer (containsNull = true)\n",
      " |    |    |-- geo: struct (nullable = true)\n",
      " |    |    |    |-- lat: double (nullable = true)\n",
      " |    |    |    |-- long: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Printing the schema of dataframe\n",
    "Data_Frame.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+--------------------+\n",
      "| dc_id|          key|               value|\n",
      "+------+-------------+--------------------+\n",
      "|dc-101|sensor-igauge|[Sensor attached ...|\n",
      "|dc-101|  sensor-ipad|[Sensor ipad atta...|\n",
      "|dc-101| sensor-inest|[Sensor attached ...|\n",
      "|dc-101|sensor-istick|[Sensor embedded ...|\n",
      "+------+-------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Using the explode functions to explode the column source into individual columns\n",
    "#As the column source has values corresponding to the string(sensor-igauge,etc)\n",
    "#Key and Values as we have defigned map in our schema structure\n",
    "exploded_source_df = Data_Frame.select(\"dc_id\", explode(\"source\"))\n",
    "exploded_source_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+---------------+---------+------------------+--------------------+\n",
      "| dc_id|          key|             ip|device_id|        c02_levels|                temp|\n",
      "+------+-------------+---------------+---------+------------------+--------------------+\n",
      "|dc-101|sensor-igauge|    68.28.91.22|       10|[1475, 1476, 1473]|[35, 35, 35, 36, ...|\n",
      "|dc-101|  sensor-ipad|    67.185.72.1|       13|[1370, 1371, 1372]|[45, 45, 45, 46, ...|\n",
      "|dc-101| sensor-inest|208.109.163.218|        8|[1346, 1345, 1343]|[40, 40, 40, 40, ...|\n",
      "|dc-101|sensor-istick| 204.116.105.67|        5|[1574, 1570, 1576]|[30, 30, 30, 30, ...|\n",
      "+------+-------------+---------------+---------+------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Now working with the value column(which is a struct) and it has various fields\n",
    "devices_df = exploded_source_df.select(\"dc_id\", \"key\",\"value.ip\",col(\"value.id\").alias(\"device_id\"),\n",
    "                        col(\"value.c02_level\").alias(\"c02_levels\"),\n",
    "                        \"value.temp\")\n",
    "devices_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dc_id: string (nullable = true)\n",
      " |-- key: string (nullable = false)\n",
      " |-- ip: string (nullable = true)\n",
      " |-- device_id: integer (nullable = true)\n",
      " |-- c02_levels: array (nullable = true)\n",
      " |    |-- element: integer (containsNull = true)\n",
      " |-- temp: array (nullable = true)\n",
      " |    |-- element: integer (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "devices_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now ceating a temporary table or the view of table\n",
    "#If you want to have a temporary view that is shared among all sessions and keep alive \n",
    "#until the Spark application terminates, you can create a global temporary view.\n",
    "#Global temporary view is tied to a system preserved database global_temp.\n",
    "devices_df.createOrReplaceTempView(\"iot_devices\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+---------------+---------+------------------+--------------------+\n",
      "| dc_id|          key|             ip|device_id|        c02_levels|                temp|\n",
      "+------+-------------+---------------+---------+------------------+--------------------+\n",
      "|dc-101|sensor-igauge|    68.28.91.22|       10|[1475, 1476, 1473]|[35, 35, 35, 36, ...|\n",
      "|dc-101|  sensor-ipad|    67.185.72.1|       13|[1370, 1371, 1372]|[45, 45, 45, 46, ...|\n",
      "|dc-101| sensor-inest|208.109.163.218|        8|[1346, 1345, 1343]|[40, 40, 40, 40, ...|\n",
      "|dc-101|sensor-istick| 204.116.105.67|        5|[1574, 1570, 1576]|[30, 30, 30, 30, ...|\n",
      "+------+-------------+---------------+---------+------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Using higher order SQL functions\n",
    "df1 = spark.sql(\"select * from iot_devices\")\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, dc_id: string, key: string, ip: string, device_id: string]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#descriptions of df1 dataframe\n",
    "#Consist of 5 columns and all with datatype string\n",
    "df1.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Higher order functions and lambda expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------------+--------------------+--------------------+\n",
      "|          key|             ip|                temp|               f_tem|\n",
      "+-------------+---------------+--------------------+--------------------+\n",
      "|sensor-igauge|    68.28.91.22|[35, 35, 35, 36, ...|[95, 95, 95, 96, ...|\n",
      "|  sensor-ipad|    67.185.72.1|[45, 45, 45, 46, ...|[113, 113, 113, 1...|\n",
      "| sensor-inest|208.109.163.218|[40, 40, 40, 40, ...|[104, 104, 104, 1...|\n",
      "|sensor-istick| 204.116.105.67|[30, 30, 30, 30, ...|[86, 86, 86, 86, ...|\n",
      "+-------------+---------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Using the transform function\n",
    "#Basic structure of transform function is \"transform(values, value -> lambda expression)\"\n",
    "#Takes an array and a anonymous function as input(transform apply fn to each element of array)\n",
    "#we can specify multiple arguments by creating a comma separated list of arguments enclosed by parenthesis like (x, y) -> x + y.\n",
    "df = spark.sql(\"select key, ip,temp,transform (temp, t -> ((t * 9) div 5) + 32 ) as f_tem from iot_devices\")\n",
    "#Converting temperature from celsius to fahrenheit scale\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+------------------+--------------------+\n",
      "|          key|device_id|        c02_levels|      low_c02_levels|\n",
      "+-------------+---------+------------------+--------------------+\n",
      "|sensor-igauge|       10|[1475, 1476, 1473]|[false, false, fa...|\n",
      "|  sensor-ipad|       13|[1370, 1371, 1372]|[false, false, fa...|\n",
      "| sensor-inest|        8|[1346, 1345, 1343]|[false, false, fa...|\n",
      "|sensor-istick|        5|[1574, 1570, 1576]|[false, false, fa...|\n",
      "+-------------+---------+------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Changing the CO2 levels in iot_devices\n",
    "spark.sql(\"select key,device_id,c02_levels,transform(c02_levels,t->t<500) as low_c02_levels from iot_devices\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+------------------+------------------+\n",
      "|          key|device_id|        c02_levels|   high_c02_levels|\n",
      "+-------------+---------+------------------+------------------+\n",
      "|sensor-igauge|       10|[1475, 1476, 1473]|                []|\n",
      "|  sensor-ipad|       13|[1370, 1371, 1372]|                []|\n",
      "| sensor-inest|        8|[1346, 1345, 1343]|                []|\n",
      "|sensor-istick|        5|[1574, 1570, 1576]|[1574, 1570, 1576]|\n",
      "+-------------+---------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Using Filters(filter and transform has nearly same syntax)\n",
    "#Unlike transform() with a boolean expression,filter produces an output array from an input array by only adding elements for which predicate function<T, Boolean> holds\n",
    "spark.sql(\"select key,device_id,c02_levels,filter(c02_levels,t->t>1500) as high_c02_levels from iot_devices\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically the main thing is understanding the difference between the transform function and filter function \n",
    "As out transform function return a boolean array having true and false as values in case when we apply some condition \n",
    "of > or < but at the same time filter returns an array of the values which satisfy the condition(used to filter\n",
    "the input based on a condition and retrieve values)->this is the main concern here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+--------------------+------------------+---------------+\n",
      "|          key|device_id|                temp|        c02_levels|existing_values|\n",
      "+-------------+---------+--------------------+------------------+---------------+\n",
      "|sensor-igauge|       10|[35, 35, 35, 36, ...|[1475, 1476, 1473]|          false|\n",
      "|  sensor-ipad|       13|[45, 45, 45, 46, ...|[1370, 1371, 1372]|          false|\n",
      "| sensor-inest|        8|[40, 40, 40, 40, ...|[1346, 1345, 1343]|          false|\n",
      "|sensor-istick|        5|[30, 30, 30, 30, ...|[1574, 1570, 1576]|           true|\n",
      "+-------------+---------+--------------------+------------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Using exists function\n",
    "#Return true if any one of the element in the array meets the required condition.\n",
    "#exists(array<T>, function<T, V, Boolean>) Boolean Return true if predicate function<T, Boolean> holds for any element in input array\n",
    "spark.sql(\"select key,device_id,temp,c02_levels,exists(c02_levels,t->t>1500)as existing_values from iot_devices\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the reduce function\n",
    "#Aggregate function is similar as reduce function\n",
    "#Reduce function allows aggregation\n",
    "#reduce(array<T>, B, function<B, T, B>, function<B, R>)R Reduce the elements of array<T> into a single value R by merging the elements into a buffer B using function<B, T, B> and by applying a finish function<B, R> on the final buffer. The initial value B is determined by a zero expression\n",
    "#The finalize function is optional, if you do not specify the function the finalize function the identity function (id -> id) is used. This is the only higher-order function that takes two lambda functions.\n",
    "spark.sql(\"select key,ip,device_id,c02_levels,reduce(c02_levels, 0, (t, acc)->t + acc,acc->acc div size(c02_levels)) as average_c02_levels from iot_devices sort by average_c02_levels desc\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the built in functions for arrays and strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------+\n",
      "|array_distinct(array(1, 2, 3, 3, 1, 2, 3, 4, 5, 6))|\n",
      "+---------------------------------------------------+\n",
      "|                                 [1, 2, 3, 4, 5, 6]|\n",
      "+---------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Using array_distinct\n",
    "#Shows only the distinct element\n",
    "spark.sql(\"select array_distinct(array(1,2,3,3,1,2,3,4,5,6))\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------+\n",
      "|array_intersect(array(1, 2, 3), array(1, 3, 5))|\n",
      "+-----------------------------------------------+\n",
      "|                                         [1, 3]|\n",
      "+-----------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Using array_intersects\n",
    "spark.sql(\"select array_intersect(array(1,2,3), array(1,3,5))\").show()\n",
    "#Similarly we can use union of two arrays\n",
    "#use array_except to return elements which are in array 1 but not in array 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using array_join\n",
    "#Concatenates the elements of the given array using the delimiter and an optional string to replace nulls. \n",
    "#If no value is set for null replacement, any null value is filtered.\n",
    "spark.sql(\"select array_join(array(\"a\", \"b\", \"c\"),\" \",\",\")\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Similarly we can find max ,min,position of an element,remove elements ,sort etc\n",
    "#Array overlaps return true if any of the element matches in both the arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can use concat to concatenate the arrays or strings also we can use flatten to transform array of arrays into single array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|zip_with(array(1, 2, 3), array(a, b, c), lambdafunction(named_struct(y, namedlambdavariable(), x, namedlambdavariable()), namedlambdavariable(), namedlambdavariable()))|\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|                                                                                                                                                    [[a, 1], [b, 2], ...|\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Using the zip function\n",
    "spark.sql(\"select zip_with(array(1, 2, 3), array('a', 'b', 'c'), (x, y) -> (y, x))\").show()\n",
    "#Reversing the locations of x and y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|zip_with(array(a, b, c), array(1, 2, 2), lambdafunction(concat(namedlambdavariable(), namedlambdavariable()), namedlambdavariable(), namedlambdavariable()))|\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|                                                                                                                                                [a1, b2, c2]|\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select zip_with(array('a', 'b', 'c'), array('1', '2', '2'), (x, y) -> concat(x, y))\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Also there are several other array function to easy the things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
